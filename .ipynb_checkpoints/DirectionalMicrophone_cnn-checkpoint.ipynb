{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.cuda as cuda\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pdb\n",
    "import pickle \n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd \n",
    "import scipy.io as sio\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuda.check_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the dataDirectory and the path for saving data as .npy \n",
    "numSubjects = 17\n",
    "dataDir\n",
    "npyDataDir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the name of the files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the name of the files \n",
    "# the form is FARAH_sub0XY_LS(1/4)_tr(1:4)\n",
    "# 16 subjects with LS1 and LS4, subject 17 has only LS1\n",
    "# LS1:Loud speaker to the left \n",
    "# LS4:Loud speaker to the right \n",
    "fns = []\n",
    "labels = []\n",
    "for subject in range(1,numSubjects+1):\n",
    "    for spk in [1,4]:\n",
    "        for trial in range(1,4+1):\n",
    "            dt = []\n",
    "            fn = 'FARAH_'+'sub0%0.2d'%subject+'_LS'+str(spk)+'_'+str(trial)\n",
    "            fns.append(fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### generate name of the subject based on the speaker side and the subject number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we give only the sujbect number and left or right speaker, and it generated the list of names\n",
    "def gen_nameLists(subjectNumber, speakerSide):\n",
    "    ssides = [1,4]\n",
    "    if speakerSide not in ssides:\n",
    "        print(\"speaker side is 1 or 4\")\n",
    "        #break\n",
    "    \n",
    "    namelists = []\n",
    "    for trial in range(1,5):\n",
    "        fname = 'FARAH_'+'sub0%0.2d'%subjectNumber+'_LS'+str(speakerSide)+'_'+str(trial)\n",
    "        namelists.append(fname)\n",
    "    return namelists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_by_name(fname):\n",
    "    fpath = dataDir + fname\n",
    "    dt = sio.loadmat(fpath)['y']\n",
    "    dt = dt.transpose()\n",
    "    return np.delete(dt, 29, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving each subject as a separate npy file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing the individual files in a csv file\n",
    "for fn in fns:\n",
    "    dt = read_by_name(fn)\n",
    "    path = npyDataDir +fn+'.npy'\n",
    "    np.save(path, dt)\n",
    "    # pd.DataFrame(dt).to_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the dataset labels as a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_labels = dict()\n",
    "dict_electrode_labels = dict()\n",
    "for subj in range(1,numSubjects+1):\n",
    "    \n",
    "    rspk = gen_nameLists(subj, speakerSide=1)\n",
    "    lspk = gen_nameLists(subj, speakerSide=4)\n",
    "    \n",
    "    for name in rspk:\n",
    "        dict_labels[name] = '+1'\n",
    "        \n",
    "    for name in lspk:\n",
    "        dict_labels[name] = '-1'\n",
    "\n",
    "with open(npyDataDir + 'dict.csv', 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    for key, value in dict_labels.items():\n",
    "        writer.writerow([key, value])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constructing the dataloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_labels = dict()\n",
    "dict_electrode_labels = dict()\n",
    "for subj in range(1,numSubjects+1):\n",
    "    \n",
    "    rspk = gen_nameLists(subj, speakerSide=1)\n",
    "    lspk = gen_nameLists(subj, speakerSide=4)\n",
    "    \n",
    "    for name in rspk:\n",
    "        dict_labels[name] = '+1'\n",
    "        \n",
    "    for name in lspk:\n",
    "        dict_labels[name] = '-1'\n",
    "\n",
    "with open(npyDataDir + 'dict.csv', 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    for key, value in dict_labels.items():\n",
    "        writer.writerow([key, value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DirectionalMicrophoneDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,subjects):\n",
    "        \n",
    "        # path: the path to the npy files \n",
    "        # subjects: the list of the subjects to be loaded\n",
    "        \n",
    "        self.path = npyDataDir\n",
    "        self.subjects = subjects \n",
    "        self.samples = []\n",
    "        \n",
    "        #pdb.set_trace()\n",
    "        for subject in self.subjects:\n",
    "            for spk in [1, 4]:\n",
    "                if spk == 1:\n",
    "                    label = 0 # Right\n",
    "                else:\n",
    "                    label = 1 # Left\n",
    "                \n",
    "                for trial in [1, 2, 3, 4]:\n",
    "                    fname = 'FARAH_'+'sub0%0.2d'%subject+'_LS'+str(spk)+'_'+str(trial)\n",
    "                    self.samples.append({\n",
    "                        'fname': fname,\n",
    "                        'label': label\n",
    "                    })\n",
    "                \n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, indx):\n",
    "        # \n",
    "        itempath = self.path + self.samples[indx]['fname'] +'.npy'\n",
    "        signal = np.load(itempath) # loads the data \n",
    "        signal_label = self.samples[indx]['label'] # the corresponding label \n",
    "        \n",
    "        return {'signal': signal, \n",
    "               'label': signal_label }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomWindow(object):\n",
    "    def __init__(self, window_size):\n",
    "        self.window_size = window_size\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        signal, label = sample['signal'], sample['label']\n",
    "        \n",
    "        # generate a random interval \n",
    "        random_index = np.random.randint(signal.shape[0]- self.window_size)\n",
    "        dt = signal[random_index:random_index+self.window_size,:]\n",
    "        return {'signal': dt, 'label': label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    def __call__(self, sample):\n",
    "        signal, label = sample['signal'], sample['label']\n",
    "        dt = torch.from_numpy(signal.transpose())\n",
    "        return {'signal': dt, 'label': label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformedDataset(Dataset):\n",
    "    def __init__(self, dataset, *transforms):\n",
    "        self.dataset = dataset\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataset[idx]\n",
    "        for transform in self.transforms:\n",
    "            sample = transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_subjects = range(1,16)\n",
    "dmd = DirectionalMicrophoneDataset(list_subjects)\n",
    "window = RandomWindow(10000)\n",
    "to_tensor = ToTensor()\n",
    "dataset = TransformedDataset(dmd, window, to_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_subject = [17]\n",
    "valid_dt= DirectionalMicrophoneDataset(valid_subject)\n",
    "#to_tensor = ToTensor()\n",
    "valid_dataset = TransformedDataset(valid_dt, window, to_tensor)\n",
    "valid_dl = DataLoader(valid_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'fname': 'FARAH_sub017_LS1_1', 'label': 0},\n",
       " {'fname': 'FARAH_sub017_LS1_2', 'label': 0},\n",
       " {'fname': 'FARAH_sub017_LS1_3', 'label': 0},\n",
       " {'fname': 'FARAH_sub017_LS1_4', 'label': 0},\n",
       " {'fname': 'FARAH_sub017_LS4_1', 'label': 1},\n",
       " {'fname': 'FARAH_sub017_LS4_2', 'label': 1},\n",
       " {'fname': 'FARAH_sub017_LS4_3', 'label': 1},\n",
       " {'fname': 'FARAH_sub017_LS4_4', 'label': 1}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dataset.dataset.samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_dataset.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the network structure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DMNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Convolution Layer 1\n",
    "        self.conv1 = nn.Conv1d(in_channels=127, out_channels=12, kernel_size=12)\n",
    "        self.norm1 = nn.BatchNorm1d(12)\n",
    "        self.maxpool1 = nn.MaxPool1d(2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        # Convolution Layer 2\n",
    "        self.conv2 = nn.Conv1d(in_channels=12, out_channels=25, kernel_size=25)\n",
    "        self.norm2 = nn.BatchNorm1d(25)\n",
    "        self.maxpool2 = nn.MaxPool1d(2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        # Convolution Layer 3\n",
    "        self.conv3 = nn.Conv1d(in_channels=25, out_channels=1, kernel_size=10)\n",
    "        self.norm3 = nn.BatchNorm1d(1)\n",
    "        self.maxpool3 = nn.MaxPool1d(4)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(619, 100)\n",
    "        self.fc2 = nn.Linear(100, 2)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.relu2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.norm3(x)\n",
    "        x = self.maxpool3(x)\n",
    "        x = self.relu3(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully connected layer 1\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=True)\n",
    "        \n",
    "        # Fully connected layer 2\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the object and defining the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model\n",
    "net = DMNetwork()\n",
    "\n",
    "if cuda.is_available():\n",
    "    print(\"cuda available\")\n",
    "    net = net.cuda()\n",
    "\n",
    "# Our loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Our optimizer\n",
    "learning_rate = 0.0001\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training and validation steps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2000\n",
    "\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "train_accuracy = []\n",
    "valid_accuracy = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    ############################\n",
    "    # Train\n",
    "    ############################    \n",
    "    iter_loss = 0.0\n",
    "    correct = 0\n",
    "    iterations = 0\n",
    "    \n",
    "    net.train()                   # Put the network into training mode\n",
    "    \n",
    "    for samples in train_dl:\n",
    "        # Convert torch tensor to Variable\n",
    "        items = Variable(samples['signal'])\n",
    "        classes = Variable(samples['label'])\n",
    "        \n",
    "        # If we have GPU, shift the data to GPU\n",
    "        if cuda.is_available():\n",
    "            items = items.cuda()\n",
    "            classes = classes.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()     # Clear off the gradients from any past operation\n",
    "        outputs = net(items.float())      # Do the forward pass\n",
    "        loss = criterion(outputs, classes) # Calculate the loss\n",
    "        #pdb.set_trace()\n",
    "        #iter_loss += loss.data[0] # Accumulate the loss\n",
    "        iter_loss += loss.data.item()\n",
    "        loss.backward()           # Calculate the gradients with help of back propagation\n",
    "        optimizer.step()          # Ask the optimizer to adjust the parameters based on the gradients\n",
    "        \n",
    "        # Record the correct predictions for training data \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct += (predicted == classes.data).sum()\n",
    "        iterations += 1\n",
    "    \n",
    "    # Record the training loss\n",
    "    train_loss.append(iter_loss/iterations)\n",
    "    # Record the training accuracy\n",
    "    #train_accuracy.append((100 * correct / len(mnist_train_loader.dataset)))\n",
    "    print(\"Error: %.2f\" % (iter_loss/iterations)+ \"---- Accuracy: %.2f%%\" % (100 * correct / len(train_dl.dataset)))\n",
    "    #print(\"Accuracy: %.2f%%\" % (100 * correct / len(train_dl.dataset)))\n",
    "   \n",
    "\n",
    "    # computing the validation loss \n",
    "    loss = 0.0\n",
    "    correct = 0\n",
    "    iterations = 0\n",
    "    \n",
    "    # putting the network into evaluation mode \n",
    "    net.eval()\n",
    "    \n",
    "    for validSample in valid_dl:\n",
    "        \n",
    "        items = Variable(validSample['signal'])\n",
    "        classes = Variable(validSample['label'])\n",
    "        \n",
    "        # If we have GPU, shift the data to GPU\n",
    "        if cuda.is_available():\n",
    "            items = items.cuda()\n",
    "            classes = classes.cuda()\n",
    "        \n",
    "        outputs = net(items.float())\n",
    "        loss += criterion(outputs, classes).data.item()\n",
    "        #loss += criterion(outputs, classes).data[0]\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct += (predicted == classes.data).sum()\n",
    "        \n",
    "        iterations  +=1 \n",
    "        \n",
    "        \n",
    "    valid_loss.append(loss/iterations)\n",
    "    # Record the validation accuracy\n",
    "    valid_accuracy.append(correct / len(valid_dl.dataset) * 100.0)\n",
    "    \n",
    "    print ('Val Loss: %.4f,---- Val Acc: %.4f' %(valid_loss[-1], valid_accuracy[-1]))\n",
    "    pdb.set_trace()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.0.dev20190326'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
