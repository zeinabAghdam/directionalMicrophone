{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DirectionalMicrophone_cnn: so that it detects for every subject \n",
    "# We leave one trial out per subject, train over the rest of the trials, and test at the end for the left out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.cuda as cuda\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pdb\n",
    "import pickle \n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import random \n",
    "import pandas as pd \n",
    "import scipy.io as sio\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "numSubjects = 17\n",
    "dataDir = \"/home/zeinab.schaefer/DMfiles/\"\n",
    "npyDataDir = \"/home/zeinab.schaefer/codes/dataDM/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the name of the files \n",
    "# the form is FARAH_sub0XY_LS(1/4)_tr(1:4)\n",
    "# 16 subjects with LS1 and LS4, subject 17 has only LS1\n",
    "# LS1:Loud speaker to the left \n",
    "# LS4:Loud speaker to the right \n",
    "fns = []\n",
    "labels = []\n",
    "for subject in range(1,numSubjects+1):\n",
    "    for spk in [1,4]:\n",
    "        for trial in range(1,4+1):\n",
    "            dt = []\n",
    "            fn = 'FARAH_'+'sub0%0.2d'%subject+'_LS'+str(spk)+'_'+str(trial)\n",
    "            fns.append(fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### generate name of the subject based on the speaker side and the subject number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_by_name(fname):\n",
    "    fpath = dataDir + fname\n",
    "    dt = sio.loadmat(fpath)['y']\n",
    "    dt = dt.transpose()\n",
    "    return np.delete(dt, 29, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_by_spk_tr(subject, spk, trial_number):\n",
    "    # generate the name\n",
    "    fn = 'FARAH_'+'sub0%0.2d'%subject+'_LS'+str(spk)+'_'+str(trial_number)\n",
    "    return fn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_nameLists(subjectNumber, speakerSide):\n",
    "    ssides = [1,4]\n",
    "    if speakerSide not in ssides:\n",
    "        print(\"speaker side is 1 or 4\")\n",
    "        #break\n",
    "    \n",
    "    namelists = []\n",
    "    for trial in range(1,5):\n",
    "        fname = 'FARAH_'+'sub0%0.2d'%subjectNumber+'_LS'+str(speakerSide)+'_'+str(trial)\n",
    "        namelists.append(fname)\n",
    "    return namelists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### saving the labels as csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_labels = dict()\n",
    "dict_electrode_labels = dict()\n",
    "for subj in range(1,numSubjects+1):\n",
    "    \n",
    "    rspk = gen_nameLists(subj, speakerSide=1)\n",
    "    lspk = gen_nameLists(subj, speakerSide=4)\n",
    "    \n",
    "    for name in rspk:\n",
    "        dict_labels[name] = '+1'\n",
    "        \n",
    "    for name in lspk:\n",
    "        dict_labels[name] = '-1'\n",
    "\n",
    "with open(npyDataDir + 'dict.csv', 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    for key, value in dict_labels.items():\n",
    "        writer.writerow([key, value])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "constructing the data loader \n",
    "we are taking all the subjects for training, but one trial out for each speaker for every subject for the purpose of testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_for(subject, spk, trial, label):\n",
    "    fname = 'FARAH_'+'sub0%0.2d'%subject+'_LS'+str(spk)+'_'+str(trial)\n",
    "    return {'fname': fname, 'label': label }\n",
    "\n",
    "train_samples = []\n",
    "valid_samples = []\n",
    "\n",
    "for subject in range(1,18):           \n",
    "    for spk in [1, 4]:\n",
    "        if spk == 1:\n",
    "            label = 0 # Right\n",
    "        else:\n",
    "            label = 1 # Left\n",
    "        # randomly take three trials out of four for training\n",
    "        # one trial for testing\n",
    "        all_trials = range(1,5)\n",
    "        train_trials = random.sample(all_trials, 3)\n",
    "        valid_trials = np.setdiff1d(all_trials, train_trials)\n",
    "        \n",
    "        # list of validation names accordingly\n",
    "        for trial in valid_trials:\n",
    "            valid_samples.append(sample_for(subject, spk, trial, label))\n",
    "        \n",
    "        for trial in train_trials:\n",
    "            train_samples.append(sample_for(subject, spk, trial, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DirectionalMicrophoneDataset(Dataset):\n",
    "    def __init__(self,samples):\n",
    "        # path: the path to the npy files \n",
    "        # subjects: the list of the subjects to be loaded    \n",
    "        self.path = npyDataDir \n",
    "        self.samples = samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, indx):\n",
    "        itempath = self.path + self.samples[indx]['fname'] +'.npy'\n",
    "        signal = np.load(itempath) # loads the data \n",
    "        signal_label = self.samples[indx]['label'] # the corresponding label \n",
    "        return {'signal': signal, 'label': signal_label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomWindow(object):\n",
    "    def __init__(self, window_size):\n",
    "        self.window_size = window_size\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        signal, label = sample['signal'], sample['label']\n",
    "        \n",
    "        # generate a random interval \n",
    "        random_index = np.random.randint(signal.shape[0] - self.window_size)\n",
    "        dt = signal[random_index:random_index+self.window_size,:]\n",
    "        return {'signal': dt, 'label': label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    def __call__(self, sample):\n",
    "        signal, label = sample['signal'], sample['label']\n",
    "        dt = torch.from_numpy(signal.transpose())\n",
    "        return {'signal': dt, 'label': label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformedDataset(Dataset):\n",
    "    def __init__(self, dataset, *transforms):\n",
    "        self.dataset = dataset\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataset[idx]\n",
    "        for transform in self.transforms:\n",
    "            sample = transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing the validation and training data loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmd = DirectionalMicrophoneDataset(train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = RandomWindow(1000)\n",
    "to_tensor = ToTensor()\n",
    "dataset_train = TransformedDataset(dmd, window, to_tensor)\n",
    "train_dl = DataLoader(dataset_train, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmd_valid = DirectionalMicrophoneDataset(valid_samples)\n",
    "valid_dataset = TransformedDataset(dmd_valid, window, to_tensor)\n",
    "valid_dl = DataLoader(valid_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing the Network Structure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1d(in_channels, out_channels, kernel_size, pool_size=1):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size),\n",
    "        nn.BatchNorm1d(out_channels),\n",
    "        nn.MaxPool1d(pool_size),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "\n",
    "class DMNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.convs = nn.Sequential(\n",
    "            conv1d(127, 64, kernel_size=32, pool_size=4),\n",
    "            conv1d(64, 34, kernel_size=16),\n",
    "            conv1d(34, 16, kernel_size=8)\n",
    "        )\n",
    "        \n",
    "        self.pool = nn.AdaptiveAvgPool1d(8)\n",
    "        self.fc1 = nn.Linear(16*8, 100)\n",
    "        self.fc2 = nn.Linear(100, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.convs(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully connected layer 1\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=True)\n",
    "        \n",
    "        # Fully connected layer 2\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available\n"
     ]
    }
   ],
   "source": [
    "# The model\n",
    "net = DMNetwork()\n",
    "\n",
    "if cuda.is_available():\n",
    "    print(\"cuda available\")\n",
    "    net = net.cuda()\n",
    "else:\n",
    "    net = net.cpu()\n",
    "\n",
    "# Our loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Our optimizer\n",
    "learning_rate = 0.0001\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Validation set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0.86---- Accuracy: 77.45%\n",
      "Val Loss: 1.1815,---- Val Acc: 52.0588\n",
      "Error: 0.70---- Accuracy: 77.55%\n",
      "Val Loss: 0.9242,---- Val Acc: 56.7647\n",
      "Error: 0.65---- Accuracy: 75.78%\n",
      "Val Loss: 1.0955,---- Val Acc: 54.4118\n",
      "Error: 0.53---- Accuracy: 77.16%\n",
      "Val Loss: 0.7919,---- Val Acc: 56.1765\n",
      "Error: 0.47---- Accuracy: 81.08%\n",
      "Val Loss: 1.0289,---- Val Acc: 53.8235\n",
      "Error: 0.50---- Accuracy: 79.51%\n",
      "Val Loss: 0.8361,---- Val Acc: 54.4118\n",
      "Error: 0.49---- Accuracy: 78.73%\n",
      "Val Loss: 0.7362,---- Val Acc: 56.1765\n",
      "Error: 0.48---- Accuracy: 80.00%\n",
      "Val Loss: 0.7246,---- Val Acc: 52.6471\n",
      "Error: 0.45---- Accuracy: 80.10%\n",
      "Val Loss: 0.7740,---- Val Acc: 54.1176\n",
      "Error: 0.42---- Accuracy: 79.90%\n",
      "Val Loss: 0.8791,---- Val Acc: 51.1765\n",
      "Error: 0.48---- Accuracy: 80.49%\n",
      "Val Loss: 0.6631,---- Val Acc: 54.7059\n",
      "Error: 0.40---- Accuracy: 83.33%\n",
      "Val Loss: 0.9495,---- Val Acc: 57.6471\n",
      "Error: 0.45---- Accuracy: 79.90%\n",
      "Val Loss: 0.8863,---- Val Acc: 52.0588\n",
      "Error: 0.42---- Accuracy: 81.57%\n",
      "Val Loss: 0.9752,---- Val Acc: 52.9412\n",
      "Error: 0.40---- Accuracy: 80.20%\n",
      "Val Loss: 1.1720,---- Val Acc: 51.1765\n",
      "Error: 0.44---- Accuracy: 79.12%\n",
      "Val Loss: 1.0348,---- Val Acc: 52.9412\n",
      "Error: 0.45---- Accuracy: 80.49%\n",
      "Val Loss: 1.1057,---- Val Acc: 49.4118\n",
      "Error: 0.39---- Accuracy: 81.08%\n",
      "Val Loss: 0.8472,---- Val Acc: 54.7059\n",
      "Error: 0.45---- Accuracy: 80.98%\n",
      "Val Loss: 1.4372,---- Val Acc: 55.2941\n",
      "Error: 0.40---- Accuracy: 83.33%\n",
      "Val Loss: 0.9781,---- Val Acc: 55.5882\n",
      "Error: 0.42---- Accuracy: 81.57%\n",
      "Val Loss: 0.7920,---- Val Acc: 56.1765\n",
      "Error: 0.42---- Accuracy: 80.59%\n",
      "Val Loss: 0.8887,---- Val Acc: 55.2941\n",
      "Error: 0.41---- Accuracy: 81.37%\n",
      "Val Loss: 0.7661,---- Val Acc: 52.9412\n",
      "Error: 0.38---- Accuracy: 83.53%\n",
      "Val Loss: 0.8094,---- Val Acc: 52.6471\n",
      "Error: 0.42---- Accuracy: 81.18%\n",
      "Val Loss: 1.1975,---- Val Acc: 56.7647\n",
      "Error: 0.45---- Accuracy: 81.57%\n",
      "Val Loss: 0.7361,---- Val Acc: 55.8824\n",
      "Error: 0.43---- Accuracy: 80.39%\n",
      "Val Loss: 1.0767,---- Val Acc: 55.5882\n",
      "Error: 0.38---- Accuracy: 83.43%\n",
      "Val Loss: 0.9035,---- Val Acc: 55.0000\n",
      "Error: 0.42---- Accuracy: 82.45%\n",
      "Val Loss: 0.8711,---- Val Acc: 53.8235\n",
      "Error: 0.37---- Accuracy: 82.35%\n",
      "Val Loss: 0.8919,---- Val Acc: 52.6471\n",
      "Error: 0.41---- Accuracy: 81.27%\n",
      "Val Loss: 0.8226,---- Val Acc: 55.8824\n",
      "Error: 0.43---- Accuracy: 82.06%\n",
      "Val Loss: 0.7725,---- Val Acc: 52.0588\n",
      "Error: 0.42---- Accuracy: 82.16%\n",
      "Val Loss: 0.7133,---- Val Acc: 51.7647\n",
      "Error: 0.39---- Accuracy: 82.25%\n",
      "Val Loss: 0.9381,---- Val Acc: 54.7059\n",
      "Error: 0.42---- Accuracy: 82.84%\n",
      "Val Loss: 1.0136,---- Val Acc: 52.3529\n",
      "Error: 0.43---- Accuracy: 80.29%\n",
      "Val Loss: 0.8377,---- Val Acc: 55.2941\n",
      "Error: 0.40---- Accuracy: 82.84%\n",
      "Val Loss: 0.8838,---- Val Acc: 55.8824\n",
      "Error: 0.39---- Accuracy: 83.43%\n",
      "Val Loss: 0.7618,---- Val Acc: 51.1765\n",
      "Error: 0.40---- Accuracy: 83.53%\n",
      "Val Loss: 1.0135,---- Val Acc: 55.5882\n",
      "Error: 0.39---- Accuracy: 83.92%\n",
      "Val Loss: 0.7831,---- Val Acc: 52.6471\n",
      "Error: 0.38---- Accuracy: 83.82%\n",
      "Val Loss: 0.9660,---- Val Acc: 52.9412\n",
      "Error: 0.39---- Accuracy: 85.39%\n",
      "Val Loss: 0.9822,---- Val Acc: 52.3529\n",
      "Error: 0.44---- Accuracy: 82.65%\n",
      "Val Loss: 0.8197,---- Val Acc: 51.4706\n",
      "Error: 0.46---- Accuracy: 81.27%\n",
      "Val Loss: 0.8506,---- Val Acc: 50.0000\n",
      "Error: 0.45---- Accuracy: 80.88%\n",
      "Val Loss: 0.8930,---- Val Acc: 49.7059\n",
      "Error: 0.38---- Accuracy: 82.94%\n",
      "Val Loss: 1.0325,---- Val Acc: 55.0000\n",
      "Error: 0.39---- Accuracy: 83.24%\n",
      "Val Loss: 0.7544,---- Val Acc: 58.2353\n",
      "Error: 0.42---- Accuracy: 81.37%\n",
      "Val Loss: 1.0250,---- Val Acc: 55.2941\n",
      "Error: 0.44---- Accuracy: 81.76%\n",
      "Val Loss: 0.8984,---- Val Acc: 59.4118\n",
      "Error: 0.42---- Accuracy: 81.27%\n",
      "Val Loss: 0.7975,---- Val Acc: 53.2353\n",
      "Error: 0.35---- Accuracy: 84.71%\n",
      "Val Loss: 0.9423,---- Val Acc: 55.0000\n",
      "Error: 0.38---- Accuracy: 84.02%\n",
      "Val Loss: 0.9626,---- Val Acc: 55.8824\n",
      "Error: 0.38---- Accuracy: 83.43%\n",
      "Val Loss: 0.7931,---- Val Acc: 55.2941\n",
      "Error: 0.39---- Accuracy: 83.63%\n",
      "Val Loss: 1.1660,---- Val Acc: 50.0000\n",
      "Error: 0.38---- Accuracy: 84.31%\n",
      "Val Loss: 0.9677,---- Val Acc: 56.1765\n",
      "Error: 0.40---- Accuracy: 82.35%\n",
      "Val Loss: 0.7405,---- Val Acc: 53.2353\n",
      "Error: 0.36---- Accuracy: 83.82%\n",
      "Val Loss: 0.8562,---- Val Acc: 53.2353\n",
      "Error: 0.35---- Accuracy: 85.10%\n",
      "Val Loss: 0.9331,---- Val Acc: 49.4118\n",
      "Error: 0.42---- Accuracy: 82.06%\n",
      "Val Loss: 0.8157,---- Val Acc: 55.0000\n",
      "Error: 0.37---- Accuracy: 83.04%\n",
      "Val Loss: 0.9441,---- Val Acc: 52.6471\n",
      "Error: 0.45---- Accuracy: 79.22%\n",
      "Val Loss: 0.7619,---- Val Acc: 54.7059\n",
      "Error: 0.40---- Accuracy: 83.73%\n",
      "Val Loss: 0.8652,---- Val Acc: 53.5294\n",
      "Error: 0.45---- Accuracy: 81.37%\n",
      "Val Loss: 0.6766,---- Val Acc: 57.3529\n",
      "Error: 0.42---- Accuracy: 80.98%\n",
      "Val Loss: 0.9486,---- Val Acc: 53.8235\n",
      "Error: 0.35---- Accuracy: 85.29%\n",
      "Val Loss: 0.9161,---- Val Acc: 54.1176\n",
      "Error: 0.40---- Accuracy: 84.31%\n",
      "Val Loss: 0.8925,---- Val Acc: 54.1176\n",
      "Error: 0.40---- Accuracy: 83.73%\n",
      "Val Loss: 0.8940,---- Val Acc: 49.7059\n",
      "Error: 0.36---- Accuracy: 84.12%\n",
      "Val Loss: 1.2303,---- Val Acc: 57.3529\n",
      "Error: 0.37---- Accuracy: 83.14%\n",
      "Val Loss: 0.9057,---- Val Acc: 54.7059\n",
      "Error: 0.33---- Accuracy: 85.20%\n",
      "Val Loss: 1.2603,---- Val Acc: 51.4706\n",
      "Error: 0.38---- Accuracy: 82.75%\n",
      "Val Loss: 1.0373,---- Val Acc: 47.9412\n",
      "Error: 0.44---- Accuracy: 82.25%\n",
      "Val Loss: 0.8361,---- Val Acc: 52.0588\n",
      "Error: 0.35---- Accuracy: 84.61%\n",
      "Val Loss: 0.8755,---- Val Acc: 53.2353\n",
      "Error: 0.43---- Accuracy: 82.55%\n",
      "Val Loss: 1.0001,---- Val Acc: 55.8824\n",
      "Error: 0.39---- Accuracy: 83.33%\n",
      "Val Loss: 0.8563,---- Val Acc: 55.8824\n",
      "Error: 0.36---- Accuracy: 83.92%\n",
      "Val Loss: 0.6958,---- Val Acc: 57.6471\n",
      "Error: 0.35---- Accuracy: 84.71%\n",
      "Val Loss: 0.9006,---- Val Acc: 50.2941\n",
      "Error: 0.35---- Accuracy: 86.67%\n",
      "Val Loss: 0.9368,---- Val Acc: 52.6471\n",
      "Error: 0.36---- Accuracy: 84.51%\n",
      "Val Loss: 0.9214,---- Val Acc: 57.6471\n",
      "Error: 0.47---- Accuracy: 82.84%\n",
      "Val Loss: 0.9076,---- Val Acc: 54.4118\n",
      "Error: 0.41---- Accuracy: 83.24%\n",
      "Val Loss: 0.9396,---- Val Acc: 51.1765\n",
      "Error: 0.34---- Accuracy: 86.67%\n",
      "Val Loss: 1.0563,---- Val Acc: 52.6471\n",
      "Error: 0.33---- Accuracy: 84.41%\n",
      "Val Loss: 1.1350,---- Val Acc: 55.8824\n",
      "Error: 0.38---- Accuracy: 82.84%\n",
      "Val Loss: 0.8499,---- Val Acc: 58.8235\n",
      "Error: 0.32---- Accuracy: 85.59%\n",
      "Val Loss: 0.8324,---- Val Acc: 58.8235\n",
      "Error: 0.37---- Accuracy: 83.33%\n",
      "Val Loss: 1.3988,---- Val Acc: 55.5882\n",
      "Error: 0.40---- Accuracy: 82.65%\n",
      "Val Loss: 1.0070,---- Val Acc: 54.4118\n",
      "Error: 0.36---- Accuracy: 83.04%\n",
      "Val Loss: 0.9930,---- Val Acc: 55.8824\n",
      "Error: 0.31---- Accuracy: 86.96%\n",
      "Val Loss: 0.9533,---- Val Acc: 51.7647\n",
      "Error: 0.41---- Accuracy: 82.84%\n",
      "Val Loss: 0.7561,---- Val Acc: 55.8824\n",
      "Error: 0.40---- Accuracy: 83.43%\n",
      "Val Loss: 0.8542,---- Val Acc: 52.3529\n",
      "Error: 0.37---- Accuracy: 83.24%\n",
      "Val Loss: 1.0084,---- Val Acc: 53.5294\n",
      "Error: 0.34---- Accuracy: 84.31%\n",
      "Val Loss: 0.8080,---- Val Acc: 56.7647\n",
      "Error: 0.36---- Accuracy: 83.04%\n",
      "Val Loss: 1.0852,---- Val Acc: 48.8235\n",
      "Error: 0.40---- Accuracy: 82.65%\n",
      "Val Loss: 0.9559,---- Val Acc: 52.9412\n",
      "Error: 0.44---- Accuracy: 84.12%\n",
      "Val Loss: 0.9204,---- Val Acc: 56.1765\n",
      "Error: 0.36---- Accuracy: 85.59%\n",
      "Val Loss: 0.7395,---- Val Acc: 57.0588\n",
      "Error: 0.36---- Accuracy: 85.98%\n",
      "Val Loss: 0.8411,---- Val Acc: 50.5882\n",
      "Error: 0.40---- Accuracy: 81.96%\n",
      "Val Loss: 0.9336,---- Val Acc: 52.3529\n",
      "Error: 0.32---- Accuracy: 86.96%\n",
      "Val Loss: 0.9883,---- Val Acc: 52.3529\n",
      "Error: 0.39---- Accuracy: 83.53%\n",
      "Val Loss: 1.0417,---- Val Acc: 56.4706\n",
      "Error: 0.38---- Accuracy: 86.08%\n",
      "Val Loss: 0.7492,---- Val Acc: 54.1176\n",
      "Error: 0.33---- Accuracy: 85.20%\n",
      "Val Loss: 1.4491,---- Val Acc: 51.1765\n",
      "Error: 0.36---- Accuracy: 82.65%\n",
      "Val Loss: 0.8924,---- Val Acc: 56.1765\n",
      "Error: 0.39---- Accuracy: 84.31%\n",
      "Val Loss: 1.0447,---- Val Acc: 52.0588\n",
      "Error: 0.33---- Accuracy: 85.78%\n",
      "Val Loss: 0.9861,---- Val Acc: 52.0588\n",
      "Error: 0.35---- Accuracy: 86.37%\n",
      "Val Loss: 0.7869,---- Val Acc: 52.6471\n",
      "Error: 0.41---- Accuracy: 83.04%\n",
      "Val Loss: 0.8272,---- Val Acc: 54.1176\n",
      "Error: 0.37---- Accuracy: 86.08%\n",
      "Val Loss: 0.8498,---- Val Acc: 52.9412\n",
      "Error: 0.37---- Accuracy: 84.02%\n",
      "Val Loss: 0.8795,---- Val Acc: 52.9412\n",
      "Error: 0.37---- Accuracy: 82.94%\n",
      "Val Loss: 0.8981,---- Val Acc: 52.3529\n",
      "Error: 0.32---- Accuracy: 85.69%\n",
      "Val Loss: 1.0924,---- Val Acc: 57.0588\n",
      "Error: 0.33---- Accuracy: 85.20%\n",
      "Val Loss: 1.1063,---- Val Acc: 52.9412\n",
      "Error: 0.38---- Accuracy: 83.43%\n",
      "Val Loss: 1.1340,---- Val Acc: 48.5294\n",
      "Error: 0.42---- Accuracy: 83.53%\n",
      "Val Loss: 1.1746,---- Val Acc: 55.8824\n",
      "Error: 0.37---- Accuracy: 82.16%\n",
      "Val Loss: 1.0358,---- Val Acc: 54.4118\n",
      "Error: 0.33---- Accuracy: 84.90%\n",
      "Val Loss: 1.1311,---- Val Acc: 52.9412\n",
      "Error: 0.34---- Accuracy: 85.20%\n",
      "Val Loss: 1.2819,---- Val Acc: 49.7059\n",
      "Error: 0.38---- Accuracy: 84.12%\n",
      "Val Loss: 1.2312,---- Val Acc: 56.7647\n",
      "Error: 0.43---- Accuracy: 82.16%\n",
      "Val Loss: 0.9425,---- Val Acc: 48.8235\n",
      "Error: 0.41---- Accuracy: 83.04%\n",
      "Val Loss: 0.9164,---- Val Acc: 51.4706\n",
      "Error: 0.36---- Accuracy: 84.90%\n",
      "Val Loss: 1.1240,---- Val Acc: 52.3529\n",
      "Error: 0.41---- Accuracy: 84.51%\n",
      "Val Loss: 0.9315,---- Val Acc: 53.5294\n",
      "Error: 0.41---- Accuracy: 83.24%\n",
      "Val Loss: 0.7793,---- Val Acc: 54.1176\n",
      "Error: 0.36---- Accuracy: 83.14%\n",
      "Val Loss: 1.1851,---- Val Acc: 50.5882\n",
      "Error: 0.37---- Accuracy: 83.04%\n",
      "Val Loss: 0.6941,---- Val Acc: 56.1765\n",
      "Error: 0.34---- Accuracy: 83.82%\n",
      "Val Loss: 1.1818,---- Val Acc: 53.5294\n",
      "Error: 0.34---- Accuracy: 87.65%\n",
      "Val Loss: 0.9544,---- Val Acc: 55.0000\n",
      "Error: 0.39---- Accuracy: 83.92%\n",
      "Val Loss: 0.8406,---- Val Acc: 52.6471\n",
      "Error: 0.32---- Accuracy: 86.37%\n",
      "Val Loss: 1.0972,---- Val Acc: 54.4118\n",
      "Error: 0.32---- Accuracy: 85.88%\n",
      "Val Loss: 1.1071,---- Val Acc: 54.1176\n",
      "Error: 0.28---- Accuracy: 88.14%\n",
      "Val Loss: 0.9373,---- Val Acc: 54.4118\n",
      "Error: 0.38---- Accuracy: 84.41%\n",
      "Val Loss: 0.9510,---- Val Acc: 56.1765\n",
      "Error: 0.40---- Accuracy: 83.63%\n",
      "Val Loss: 0.9053,---- Val Acc: 52.3529\n",
      "Error: 0.34---- Accuracy: 85.59%\n",
      "Val Loss: 1.2450,---- Val Acc: 53.8235\n",
      "Error: 0.38---- Accuracy: 84.02%\n",
      "Val Loss: 1.1891,---- Val Acc: 55.5882\n",
      "Error: 0.34---- Accuracy: 86.27%\n",
      "Val Loss: 0.9403,---- Val Acc: 53.8235\n",
      "Error: 0.34---- Accuracy: 87.25%\n",
      "Val Loss: 1.1802,---- Val Acc: 52.3529\n",
      "Error: 0.33---- Accuracy: 85.29%\n",
      "Val Loss: 1.0305,---- Val Acc: 51.4706\n",
      "Error: 0.34---- Accuracy: 85.10%\n",
      "Val Loss: 0.9101,---- Val Acc: 57.9412\n",
      "Error: 0.36---- Accuracy: 84.22%\n",
      "Val Loss: 1.0121,---- Val Acc: 54.4118\n",
      "Error: 0.29---- Accuracy: 88.14%\n",
      "Val Loss: 1.1130,---- Val Acc: 52.6471\n",
      "Error: 0.37---- Accuracy: 86.47%\n",
      "Val Loss: 1.2812,---- Val Acc: 53.8235\n",
      "Error: 0.34---- Accuracy: 84.71%\n",
      "Val Loss: 0.9234,---- Val Acc: 55.5882\n",
      "Error: 0.33---- Accuracy: 85.00%\n",
      "Val Loss: 1.0527,---- Val Acc: 53.2353\n",
      "Error: 0.34---- Accuracy: 85.39%\n",
      "Val Loss: 1.0914,---- Val Acc: 50.0000\n",
      "Error: 0.33---- Accuracy: 85.39%\n",
      "Val Loss: 0.9643,---- Val Acc: 56.1765\n",
      "Error: 0.31---- Accuracy: 86.08%\n",
      "Val Loss: 1.0612,---- Val Acc: 55.0000\n",
      "Error: 0.34---- Accuracy: 87.06%\n",
      "Val Loss: 0.9244,---- Val Acc: 51.7647\n",
      "Error: 0.34---- Accuracy: 85.49%\n",
      "Val Loss: 1.0306,---- Val Acc: 54.7059\n",
      "Error: 0.32---- Accuracy: 85.98%\n",
      "Val Loss: 0.8562,---- Val Acc: 53.5294\n",
      "Error: 0.38---- Accuracy: 84.51%\n",
      "Val Loss: 0.9038,---- Val Acc: 50.5882\n",
      "Error: 0.35---- Accuracy: 84.12%\n",
      "Val Loss: 0.8415,---- Val Acc: 51.7647\n",
      "Error: 0.34---- Accuracy: 84.80%\n",
      "Val Loss: 0.8670,---- Val Acc: 55.2941\n",
      "Error: 0.38---- Accuracy: 84.80%\n",
      "Val Loss: 0.9249,---- Val Acc: 52.3529\n",
      "Error: 0.38---- Accuracy: 84.02%\n",
      "Val Loss: 0.9262,---- Val Acc: 49.4118\n",
      "Error: 0.36---- Accuracy: 85.10%\n",
      "Val Loss: 0.7274,---- Val Acc: 51.4706\n",
      "Error: 0.34---- Accuracy: 85.59%\n",
      "Val Loss: 0.8333,---- Val Acc: 54.7059\n",
      "Error: 0.30---- Accuracy: 86.57%\n",
      "Val Loss: 0.8948,---- Val Acc: 49.7059\n",
      "Error: 0.35---- Accuracy: 85.29%\n",
      "Val Loss: 1.0022,---- Val Acc: 50.2941\n",
      "Error: 0.36---- Accuracy: 84.22%\n",
      "Val Loss: 1.2264,---- Val Acc: 48.2353\n",
      "Error: 0.34---- Accuracy: 85.88%\n",
      "Val Loss: 1.1650,---- Val Acc: 52.3529\n",
      "Error: 0.33---- Accuracy: 86.57%\n",
      "Val Loss: 1.0814,---- Val Acc: 56.7647\n",
      "Error: 0.34---- Accuracy: 85.39%\n",
      "Val Loss: 0.9652,---- Val Acc: 54.4118\n",
      "Error: 0.31---- Accuracy: 87.25%\n",
      "Val Loss: 1.0614,---- Val Acc: 51.7647\n",
      "Error: 0.41---- Accuracy: 83.82%\n",
      "Val Loss: 1.4488,---- Val Acc: 46.1765\n",
      "Error: 0.39---- Accuracy: 83.33%\n",
      "Val Loss: 0.9471,---- Val Acc: 51.7647\n",
      "Error: 0.31---- Accuracy: 86.67%\n",
      "Val Loss: 1.2813,---- Val Acc: 50.5882\n",
      "Error: 0.33---- Accuracy: 86.27%\n",
      "Val Loss: 0.8879,---- Val Acc: 52.0588\n",
      "Error: 0.35---- Accuracy: 85.20%\n",
      "Val Loss: 0.8737,---- Val Acc: 55.2941\n",
      "Error: 0.34---- Accuracy: 87.35%\n",
      "Val Loss: 0.9708,---- Val Acc: 54.4118\n",
      "Error: 0.34---- Accuracy: 85.88%\n",
      "Val Loss: 0.9956,---- Val Acc: 56.7647\n",
      "Error: 0.32---- Accuracy: 86.96%\n",
      "Val Loss: 1.0629,---- Val Acc: 56.7647\n",
      "Error: 0.34---- Accuracy: 84.90%\n",
      "Val Loss: 1.2818,---- Val Acc: 50.8824\n",
      "Error: 0.39---- Accuracy: 84.41%\n",
      "Val Loss: 1.0834,---- Val Acc: 57.6471\n",
      "Error: 0.34---- Accuracy: 86.86%\n",
      "Val Loss: 0.8317,---- Val Acc: 55.8824\n",
      "Error: 0.33---- Accuracy: 87.35%\n",
      "Val Loss: 1.0577,---- Val Acc: 53.8235\n",
      "Error: 0.31---- Accuracy: 85.69%\n",
      "Val Loss: 1.2341,---- Val Acc: 54.1176\n",
      "Error: 0.34---- Accuracy: 87.35%\n",
      "Val Loss: 0.9424,---- Val Acc: 55.2941\n",
      "Error: 0.38---- Accuracy: 84.22%\n",
      "Val Loss: 1.2723,---- Val Acc: 57.3529\n",
      "Error: 0.35---- Accuracy: 84.12%\n",
      "Val Loss: 1.2352,---- Val Acc: 53.2353\n",
      "Error: 0.33---- Accuracy: 85.59%\n",
      "Val Loss: 1.3984,---- Val Acc: 53.2353\n",
      "Error: 0.33---- Accuracy: 86.08%\n",
      "Val Loss: 0.9353,---- Val Acc: 51.4706\n",
      "Error: 0.32---- Accuracy: 85.29%\n",
      "Val Loss: 1.2177,---- Val Acc: 52.0588\n",
      "Error: 0.32---- Accuracy: 87.45%\n",
      "Val Loss: 0.9000,---- Val Acc: 51.1765\n",
      "Error: 0.29---- Accuracy: 86.08%\n",
      "Val Loss: 1.0803,---- Val Acc: 52.6471\n",
      "Error: 0.32---- Accuracy: 86.86%\n",
      "Val Loss: 1.0100,---- Val Acc: 54.1176\n",
      "Error: 0.34---- Accuracy: 86.37%\n",
      "Val Loss: 1.0748,---- Val Acc: 52.0588\n",
      "Error: 0.35---- Accuracy: 87.25%\n",
      "Val Loss: 1.0035,---- Val Acc: 52.6471\n",
      "Error: 0.34---- Accuracy: 85.20%\n",
      "Val Loss: 0.9932,---- Val Acc: 53.2353\n",
      "Error: 0.35---- Accuracy: 87.25%\n",
      "Val Loss: 1.0414,---- Val Acc: 50.5882\n",
      "Error: 0.31---- Accuracy: 86.67%\n",
      "Val Loss: 1.2278,---- Val Acc: 55.0000\n",
      "Error: 0.34---- Accuracy: 86.08%\n",
      "Val Loss: 1.0132,---- Val Acc: 53.5294\n",
      "Error: 0.34---- Accuracy: 85.29%\n",
      "Val Loss: 0.9381,---- Val Acc: 55.5882\n",
      "Error: 0.33---- Accuracy: 87.75%\n",
      "Val Loss: 1.0816,---- Val Acc: 50.8824\n",
      "Error: 0.31---- Accuracy: 86.86%\n",
      "Val Loss: 0.9050,---- Val Acc: 53.8235\n",
      "Error: 0.35---- Accuracy: 85.88%\n",
      "Val Loss: 0.8593,---- Val Acc: 55.5882\n",
      "Error: 0.33---- Accuracy: 85.69%\n",
      "Val Loss: 1.2497,---- Val Acc: 57.6471\n",
      "Error: 0.33---- Accuracy: 85.29%\n",
      "Val Loss: 0.8001,---- Val Acc: 58.2353\n",
      "Error: 0.34---- Accuracy: 86.08%\n",
      "Val Loss: 0.9002,---- Val Acc: 56.7647\n",
      "Error: 0.30---- Accuracy: 86.76%\n",
      "Val Loss: 0.9967,---- Val Acc: 58.5294\n",
      "Error: 0.36---- Accuracy: 84.02%\n",
      "Val Loss: 0.8317,---- Val Acc: 53.5294\n",
      "Error: 0.30---- Accuracy: 87.16%\n",
      "Val Loss: 0.9987,---- Val Acc: 53.5294\n",
      "Error: 0.38---- Accuracy: 85.39%\n",
      "Val Loss: 0.9324,---- Val Acc: 52.3529\n",
      "Error: 0.31---- Accuracy: 86.96%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2000\n",
    "\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "train_accuracy = []\n",
    "valid_accuracy = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    ############################\n",
    "    # Train\n",
    "    ############################    \n",
    "    iter_loss = 0.0\n",
    "    correct = 0\n",
    "    iterations = 0\n",
    "    episodes = 10\n",
    "    \n",
    "    net.train()                   # Put the network into training mode\n",
    "    for idx in range(episodes):\n",
    "        for samples in train_dl:\n",
    "            # Convert torch tensor to Variable\n",
    "            items = Variable(samples['signal'])\n",
    "            classes = Variable(samples['label'])\n",
    "            \n",
    "            # If we have GPU, shift the data to GPU\n",
    "            if cuda.is_available():\n",
    "                items = items.cuda()\n",
    "                classes = classes.cuda()\n",
    "            else:\n",
    "                items = items.cpu()\n",
    "                classes = classes.cpu()\n",
    "            \n",
    "            optimizer.zero_grad()     # Clear off the gradients from any past operation\n",
    "            outputs = net(items.float())      # Do the forward pass\n",
    "            loss = criterion(outputs, classes) # Calculate the loss\n",
    "            #pdb.set_trace()\n",
    "            #iter_loss += loss.data[0] # Accumulate the loss\n",
    "            iter_loss += loss.data.item()\n",
    "            loss.backward()           # Calculate the gradients with help of back propagation\n",
    "            optimizer.step()          # Ask the optimizer to adjust the parameters based on the gradients\n",
    "            \n",
    "            # Record the correct predictions for training data \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == classes.data).sum()\n",
    "            iterations += 1\n",
    "    \n",
    "    # Record the training loss\n",
    "    train_loss.append(iter_loss/iterations)\n",
    "    # Record the training accuracy\n",
    "    #train_accuracy.append((100 * correct / len(mnist_train_loader.dataset)))\n",
    "    #print(\"Tr Error: %.2f\" % (iter_loss/iterations)+ \"---- Accuracy: %.2f%%\" % (100/episodes * correct.item() / len(train_dl.dataset)))\n",
    "    print ('Train Error: %.2f,----Acc: %.4f' %((iter_loss/iterations), (100/episodes * correct.item() / len(train_dl.dataset)))\n",
    "\n",
    "    \n",
    "    #print(\"Accuracy: %.2f%%\" % (100 * correct / len(train_dl.dataset)))\n",
    "   \n",
    "\n",
    "    # computing the validation loss \n",
    "    loss = 0.0\n",
    "    correct = 0\n",
    "    iterations = 0\n",
    "    \n",
    "    # putting the network into evaluation mode \n",
    "    net.eval()\n",
    "    \n",
    "    for idx in range(10):\n",
    "        for validSample in valid_dl:\n",
    "            \n",
    "            items = Variable(validSample['signal'])\n",
    "            classes = Variable(validSample['label'])\n",
    "            \n",
    "            # If we have GPU, shift the data to GPU\n",
    "            if cuda.is_available():\n",
    "                items = items.cuda()\n",
    "                classes = classes.cuda()\n",
    "            else:\n",
    "                items = items.cpu()\n",
    "                classes = classes.cpu()\n",
    "            \n",
    "            outputs = net(items.float())\n",
    "            loss += criterion(outputs, classes).data.item()\n",
    "            #loss += criterion(outputs, classes).data[0]\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == classes.data).sum()\n",
    "            iterations  +=1 \n",
    "            \n",
    "            \n",
    "    valid_loss.append(loss/iterations)\n",
    "    # Record the validation accuracy\n",
    "    #pdb.set_trace()\n",
    "    valid_accuracy.append(correct.item() / len(valid_dl.dataset) * 100.0 / episodes)\n",
    "    #pdb.set_trace()\n",
    "    print ('Val Error: %.2f,----Acc: %.4f' %(valid_loss[-1], valid_accuracy[-1]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"cnn_third_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_state_dict(torch.load(\"cnn_third_model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
